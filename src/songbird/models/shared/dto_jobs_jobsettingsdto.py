"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from .dto_jobs_cronscheduledto import DTOJobsCronScheduleDTO
from .dto_jobs_jobemailnotificationsdto import DTOJobsJobEmailNotificationsDTO
from .dto_jobs_newclusterdto import DTOJobsNewClusterDTO
from .dto_jobs_notebooktaskdto import DTOJobsNotebookTaskDTO
from .dto_jobs_sparkjartaskdto import DTOJobsSparkJarTaskDTO
from .dto_jobs_sparkpythontaskdto import DTOJobsSparkPythonTaskDTO
from .dto_jobs_sparksubmittaskdto import DTOJobsSparkSubmitTaskDTO
from dataclasses_json import Undefined, dataclass_json
from songbird import utils
from typing import List, Optional


@dataclasses.dataclass
class Libraries:
    pass


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class DTOJobsJobSettingsDTO:
    email_notifications: Optional[DTOJobsJobEmailNotificationsDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('emailNotifications'), 'exclude': lambda f: f is None }})
    existing_cluster_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('existingClusterId'), 'exclude': lambda f: f is None }})
    libraries: Optional[List[Libraries]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('libraries'), 'exclude': lambda f: f is None }})
    max_concurrent_runs: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxConcurrentRuns'), 'exclude': lambda f: f is None }})
    max_retries: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('maxRetries'), 'exclude': lambda f: f is None }})
    min_retry_interval_millis: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('minRetryIntervalMillis'), 'exclude': lambda f: f is None }})
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    new_cluster: Optional[DTOJobsNewClusterDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('newCluster'), 'exclude': lambda f: f is None }})
    notebook_task: Optional[DTOJobsNotebookTaskDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('notebookTask'), 'exclude': lambda f: f is None }})
    retry_on_timeout: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('retryOnTimeout'), 'exclude': lambda f: f is None }})
    schedule: Optional[DTOJobsCronScheduleDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('schedule'), 'exclude': lambda f: f is None }})
    spark_jar_task: Optional[DTOJobsSparkJarTaskDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkJarTask'), 'exclude': lambda f: f is None }})
    spark_python_task: Optional[DTOJobsSparkPythonTaskDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkPythonTask'), 'exclude': lambda f: f is None }})
    spark_submit_task: Optional[DTOJobsSparkSubmitTaskDTO] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('sparkSubmitTask'), 'exclude': lambda f: f is None }})
    timeout_seconds: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('timeoutSeconds'), 'exclude': lambda f: f is None }})
    

